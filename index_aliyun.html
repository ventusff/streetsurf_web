<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views. StreetSurf is a novel multi-view implicit surface reconstruction technique that is readily applicable to street view images in autonomous driving datasets like waymo.">
  <meta property="og:title" content="StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views"/>
  <meta property="og:description" content="We present a novel multi-view implicit surface reconstruction technique, termed StreetSurf, that is readily applicable to street view images in widely-used autonomous driving datasets, such as Waymo-perception sequences, without necessarily requiring LiDAR data. As neural rendering research expands rapidly, its integration into street views has started to draw interests. Existing approaches on street views either mainly focus on novel view synthesis with little exploration of the scene geometry, or rely heavily on dense LiDAR data when investigating reconstruction. Neither of them investigates multi-view implicit surface reconstruction, especially under settings without LiDAR data. Our method extends prior object-centric neural surface reconstruction techniques to address the unique challenges posed by the unbounded street views that are captured with non-object-centric, long and narrow camera trajectories. We delimit the unbounded space into three parts, close-range, distant-view and sky, with aligned cuboid boundaries, and adapt cuboid/hyper-cuboid hash-grids along with road-surface initialization scheme for finer and disentangled representation. To further address the geometric errors arising from textureless regions and insufficient viewing angles, we adopt geometric priors that are estimated using general purpose monocular models. Coupled with our implementation of efficient and fine-grained multi-stage ray marching strategy, we achieve state of the art reconstruction quality in both geometry and appearance within only one to two hours of training time with a single RTX3090 GPU for each street view sequence. Furthermore, we demonstrate that the reconstructed implicit surfaces have rich potential for various downstream tasks, including ray tracing and LiDAR simulation."/>
  <meta property="og:url" content="https://longtimenohack.com/streetsurf_web"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="streetsurf_web/static/teasers/fig_overview.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views">
  <meta name="twitter:description" content="We present a novel multi-view implicit surface reconstruction technique, termed StreetSurf, that is readily applicable to street view images in widely-used autonomous driving datasets, such as Waymo-perception sequences, without necessarily requiring LiDAR data. As neural rendering research expands rapidly, its integration into street views has started to draw interests. Existing approaches on street views either mainly focus on novel view synthesis with little exploration of the scene geometry, or rely heavily on dense LiDAR data when investigating reconstruction. Neither of them investigates multi-view implicit surface reconstruction, especially under settings without LiDAR data. Our method extends prior object-centric neural surface reconstruction techniques to address the unique challenges posed by the unbounded street views that are captured with non-object-centric, long and narrow camera trajectories. We delimit the unbounded space into three parts, close-range, distant-view and sky, with aligned cuboid boundaries, and adapt cuboid/hyper-cuboid hash-grids along with road-surface initialization scheme for finer and disentangled representation. To further address the geometric errors arising from textureless regions and insufficient viewing angles, we adopt geometric priors that are estimated using general purpose monocular models. Coupled with our implementation of efficient and fine-grained multi-stage ray marching strategy, we achieve state of the art reconstruction quality in both geometry and appearance within only one to two hours of training time with a single RTX3090 GPU for each street view sequence. Furthermore, we demonstrate that the reconstructed implicit surfaces have rich potential for various downstream tasks, including ray tracing and LiDAR simulation.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="streetsurf_web/static/teasers/fig_overview.jpg">
  <meta name="twitter:card" content="streetsurf_web/static/teasers/fig_overview.jpg">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="street view, 3d reconstruction, neural rendering, implicit surface reconstruction, lidar simulation, nerf, neus">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>StreetSurf</title>
  <link rel="icon" type="image/x-icon" href="streetsurf_web/static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="streetsurf_web/static/css/bulma.min.css">
  <link rel="stylesheet" href="streetsurf_web/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="streetsurf_web/static/css/bulma-divider.min.css">
  <link rel="stylesheet" href="streetsurf_web/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="streetsurf_web/static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="streetsurf_web/static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="streetsurf_web/static/js/fontawesome.all.min.js"></script>
  <script src="streetsurf_web/static/js/bulma-carousel.min.js"></script>
  <script src="streetsurf_web/static/js/bulma-slider.min.js"></script>
  <script src="streetsurf_web/static/js/index.js"></script>

  <!-- 3D model viewer -->
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>
  <script>
    self.ModelViewerElement = self.ModelViewerElement || {};
    self.ModelViewerElement.meshoptDecoderLocation = 'https://cdn.jsdelivr.net/npm/meshoptimizer/meshopt_decoder.js';
  </script>
  <style>
    .model-viewer-container {
      position: relative;
      width: 100%;
      /* Example: Set aspect ratio to 16:9 */
      padding-bottom: 56.25%; /* 100% * 9 / 16 */
      overflow: hidden;
    }

    model-viewer {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body" style="padding-bottom: 0;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://ventusff.github.io/" target="_blank">Jianfei Guo</a><sup>1</sup>,</span>
                <span class="author-block"><a href="https://www.researchgate.net/profile/Nianchen-Deng" target="_blank">Nianchen Deng</a><sup>1</sup>,</span>
                <span class="author-block"><a href="https://github.com/imlixinyang" target="_blank">Xinyang Li</a><sup>1</sup>,</span>
                <span class="author-block"><a href="https://scholar.google.com/citations?user=_fl950wAAAAJ" target="_blank">Yeqi Bai</a><sup>1</sup>,</span>
                <span class="author-block">Botian Shi<sup>1</sup>,</span>
                <br>
                <span class="author-block">Chiyu Wang<sup>2</sup>,</span>
                <span class="author-block">Chenjing Ding<sup>2</sup>,</span>
                <span class="author-block"><a href="https://scholar.google.com/citations?user=gurERzcAAAAJ" target="_blank">Dongliang Wang</a><sup>1,2</sup>,</span>
                <span class="author-block"><a href="https://liyikang.top/">Yikang Li</a><sup>1&dagger;</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> <a href="https://pjlab-adg.github.io/" target="_blank">ADLab</a> at <a href="https://www.shlab.org.cn/" target="_blank">Shanghai AI Lab.</a>,</span>
                    <span class="author-block"><sup>2</sup>Sensetime Research</span>
                  </div>

                  <div class="column has-text-centered">
                  <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                    <a href="https://arxiv.org/abs/2306.04988" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                      <a href="https://arxiv.org/pdf/2306.04988" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                    <!-- Supplementary PDF link -->
                  <!-- <span class="link-block">
                      <a href="streetsurf_web/static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/pjlab-ADG/neuralsim" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="streetsurf_web/static/teasers/seg134763_no_lidar.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="streetsurf_web/static/teasers/seg163453_no_lidar.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="streetsurf_web/static/teasers/seg405841_no_lidar.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video3" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="streetsurf_web/static/teasers/seg102751_no_lidar.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
      <h2 class="subtitle has-text-centered">
        Implicit multi-view 3D reconstruction of static backgrounds on <a href="https://waymo.com/open/data/perception/" target="_blank">Waymo Open Dataset</a>, <br><strong>without using any LiDAR data</strong> (using only posed images and inferred cues).
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a novel multi-view implicit surface reconstruction technique, termed <strong>StreetSurf</strong>, that is readily applicable to street view images in widely-used autonomous driving datasets, such as Waymo-perception sequences, without necessarily requiring LiDAR data.
          </p>
          <p>
            As neural rendering research expands rapidly, its integration into street views has started to draw interests. Existing approaches on street views either mainly focus on novel view synthesis with little exploration of the scene geometry, or rely heavily on dense LiDAR data when investigating reconstruction. Neither of them investigates multi-view implicit surface reconstruction, especially under settings without LiDAR data.
          </p> 
          <p>
            Our method extends prior object-centric neural surface reconstruction techniques to address the unique challenges posed by the unbounded street views that are captured with non-object-centric, long and narrow camera trajectories. We delimit the unbounded space into three parts, close-range, distant-view and sky, with aligned cuboid boundaries, and adapt cuboid/hyper-cuboid hash-grids along with road-surface initialization scheme for finer and disentangled representation. To further address the geometric errors arising from textureless regions and insufficient viewing angles, we adopt geometric priors that are estimated using general purpose monocular models.
          </p>

          <p>
            Coupled with our implementation of efficient and fine-grained multi-stage ray marching strategy, we achieve state of the art reconstruction quality in both geometry and appearance within only one to two hours of training time with a single RTX3090 GPU for each street view sequence. Furthermore, we demonstrate that the reconstructed implicit surfaces have rich potential for various downstream tasks, including ray tracing and LiDAR simulation.
          </p>
        </div>

        <img src="streetsurf_web/static/teasers/fig_overview.jpg" alt="StreetSurf Overview"/>
        <div class="content has-text-justified">
          <p>
            StreetSurf overview. By using a collection of posed images, along with an optional set of LiDAR data or monocular cues as inputs (or supervisions), we reconstruct the street-view scene through an optimization process. Each scene is divided into three parts according to the viewing distance, namely <strong>close-range</strong> (cr), <strong>distant-view</strong> (dv) and <strong>sky</strong>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- End youtube video -->

<!-- Overview -->
<section class="hero is-small">

</section>
<!-- End Overview -->



<!-- Replay and render 3D reconstructed streets -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Street-view reconstruction of autonomous driving datasets</h2>
      <div class="content has-text-justified">
        <p>
          StreetSurf primarily takes posed images as inputs and does not necessariy rely on dense LiDAR inputs. It can work well with dense LiDAR, with only sparse LiDARs, or even without any LiDAR at all.
          Below results are conducted on a widely-used autonomous driving dataset: <a href="https://waymo.com/open/data/perception/" target="_blank">Waymo Open Dataset</a>.
        </p>
      </div>

      <div class="columns is-centered">
      <div class="column is-half">
      <!-- With LiDAR -->
      <h3 class="title is-4">With LiDAR data input</h3>
      <div class="content has-text-justified">
        <p>Multi-view reconstruction using posed images and four sparser auxiliary LiDARs, while exluding the dense TOP LiDAR.</p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="streetsurf_web/static/teasers/seg100613_with_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg100613_with_lidar_mesh.jpg" alt="Marching Cube mesh of seg100613"/>
          <div class="subtitle has-text-centered">
            <p>seg100613..., marching cubes @ 0.1m</p>
          </div>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="streetsurf_web/static/teasers/seg153495_with_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg153495_with_lidar_mesh.jpg" alt="Marching Cube mesh of seg153495"/>
          <div class="subtitle has-text-centered">
            <p>seg153495..., marching cubes @ 0.1m</p>
          </div>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="streetsurf_web/static/teasers/seg106762_with_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg106762_with_lidar_mesh.jpg" alt="Marching Cube mesh of seg106762"/>
          <div class="subtitle has-text-centered">
            <p>seg106762..., marching cubes @ 0.1m</p>
          </div>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="streetsurf_web/static/teasers/seg158686_with_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg158686_with_lidar_mesh.jpg" alt="Marching Cube mesh of seg158686"/>
          <div class="subtitle has-text-centered">
            <p>seg158686..., marching cubes @ 0.1m</p>
          </div>
        </div>
        <div class="item item-video5">
          <video poster="" id="video5" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="streetsurf_web/static/teasers/seg152706_with_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg152706_with_lidar_mesh.jpg" alt="Marching Cube mesh of seg152706"/>
          <div class="subtitle has-text-centered">
            <p>seg152706..., marching cubes @ 0.1m</p>
          </div>
        </div>
        <div class="item item-video6">
          <video poster="" id="video6" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="streetsurf_web/static/teasers/seg132384_with_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg132384_with_lidar_mesh.jpg" alt="Marching Cube mesh of seg132384"/>
          <div class="subtitle has-text-centered">
            <p>seg132384..., marching cubes @ 0.1m</p>
          </div>
        </div>
      </div>
      <!-- End With LiDAR -->
      </div>

      <div class="column is-half">
      <!-- Without LiDAR -->
      <h3 class="title is-4">Without LiDAR data input</h3>
      <div class="content has-text-justified">
        <p>Multi-view reconstruction using only posed images and inferred cues from these images.</p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="streetsurf_web/static/teasers/seg134763_no_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg134763_no_lidar_mesh.jpg" alt="Marching Cube mesh of seg134763"/>
          <div class="subtitle has-text-centered">
            <p>seg134763..., marching cubes @ 0.1m</p>
          </div>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="streetsurf_web/static/teasers/seg163453_no_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg163453_no_lidar_mesh.jpg" alt="Marching Cube mesh of seg163453"/>
          <div class="subtitle has-text-centered">
            <p>seg163453..., marching cubes @ 0.1m</p>
          </div>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="streetsurf_web/static/teasers/seg405841_no_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg405841_no_lidar_mesh.jpg" alt="Marching Cube mesh of seg405841"/>
          <div class="subtitle has-text-centered">
            <p>seg405841..., marching cubes @ 0.1m</p>
          </div>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="streetsurf_web/static/teasers/seg102751_no_lidar.mp4"
            type="video/mp4">
          </video>
          <img src="streetsurf_web/static/teasers/seg102751_no_lidar_mesh.jpg" alt="Marching Cube mesh of seg102751"/>
          <div class="subtitle has-text-centered">
            <p>seg102751..., marching cubes @ 0.1m</p>
          </div>
        </div>
      </div>
      <!-- End Without LiDAR -->
      </div>
      </div>
    </div> <!-- <div class="container">-->
  </div>
</section>
<!-- End Replay and render 3D reconstructed streets -->



<!-- Reconstructed surfaces -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Extracting reconstructed surfaces</h2>
      <div class="content has-text-justified">
        <p>
          Using StreetSurf, you can obtain an continuous representation (SDF) of scene geometry that has infinitesimal granularity. Subsequently, you can extract high-resolution meshes or occupancy grids out of the reconstructed implicit surface.
        </p>
      </div>

      <!-- Marching Cube -->
      <h3 class="title is-4">Marching cubes @ 0.1(m)</h3>
      <div class="content has-text-justified">
        <p>
          Note that the meshes are heavily optimized to reduce size for online loading.
        </p>
      </div>
      <div class="columns is-centered">
        <div class="column is-half">
          <div class="model-viewer-container">
            <model-viewer alt="seg152706 with LiDAR mesh" src="streetsurf_web/static/meshes/seg152706_with_lidar_res=0.1_comp.glb" poster="streetsurf_web/static/teasers/seg152706_with_lidar_mesh.jpg" environment-image="legacy" shadow-intensity="0.3" camera-controls touch-action="pan-y" camera-orbit="200deg 80deg 25%" min-camera-orbit="auto auto 1%" max-camera-orbit="auto auto 50%" orbit-sensitivity="0.3" max-field-of-view="30deg" min-field-of-view="0deg" ></model-viewer>
          </div>
        </div>
        <div class="column is-half">
          <div class="model-viewer-container">
            <model-viewer alt="seg158686 with LiDAR mesh" src="streetsurf_web/static/meshes/seg158686_with_lidar_res=0.1_comp.glb" poster="streetsurf_web/static/teasers/seg158686_with_lidar_mesh.jpg" environment-image="legacy" shadow-intensity="0.3" camera-controls touch-action="pan-y" camera-orbit="320deg 80deg 25%" min-camera-orbit="auto auto 1%" max-camera-orbit="auto auto 50%" orbit-sensitivity="0.3" max-field-of-view="30deg" min-field-of-view="0deg" ></model-viewer>
          </div>
        </div>
      </div>
      <!-- End Marching Cube -->

      <!-- Multi-frame concat -->
      <!-- <h3 class="title is-4">Multi-frame concat of pointcloud projected with camera-rendered depth</h3> -->
      <!-- End multi-frame concat -->


    </div>
  </div>
</section>
<!-- End Reconstructed surfaces -->



<!-- Ray tracing -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Ray tracing guided by reconstructed surfaces</h2>
      <div class="content has-text-justified">
        <p>
          One of the major fallback of volume rendering is that its depth rendering is often indefinite and inaccurate. However with StreetSurf, the rendering process can be naturally guided by the reconstructed implicit surface representation.
        </p>
      </div>

      <!-- Lidar simulation -->
      <h3 class="title is-4">Lidar simulation</h3>
      <div class="columns is-centered">
        <!-- gt pred -->
        <div class="column is-two-thirds">
          <div class="content has-text-justified">
            <p>
              The comparison of ground truth TOP LiDAR pointcloud with the simulated pointcloud projected from the volume-rendered depths on the original LiDAR beams.
            </p>
          </div>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="streetsurf_web/static/teasers/seg405841_lidarsim_crf35.mp4" type="video/mp4">
          </video>
        </div>

        <div class="divider is-vertical is-left"></div>

        <!-- segmentation -->
        <div class="column">
          <div class="content has-text-justified">
            <p>
              Directly feeding the volume-rendered pointcloud into a 3d segmentation model obtained from <a href="https://github.com/PJLab-ADG/PCSeg" target="_blank">PCSeg</a>.
            </p>
            <img src="streetsurf_web/static/teasers/segmentation.jpg" alt="Inferred 3d segmentation on the volume-rendered pointcloud."/>
          </div>

        </div>
      </div>




      

      <h3 class="title is-4">Sphere tracing</h3>
      <div class="content has-text-justified">
        <p>
          Direcly apply sphere-tracing instead of volume rendering.
        </p>
      </div>
      <div class="columns">
        <div class="column is-one-quater">
          <img src="streetsurf_web/static/teasers/raytrace/seg134763_19_gt.jpg" alt="GT Image"/>
          <div class="content has-text-centered">
            <p>GT Image</p>
          </div>
        </div>
        <div class="column is-one-quater">
          <img src="streetsurf_web/static/teasers/raytrace/seg134763_19_vol_normal.jpg" alt="Volume-rendered normals"/>
          <div class="content has-text-centered">
            <p>Volume-rendered normals</p>
          </div>
        </div>
        <div class="column is-one-quater">
          <img src="streetsurf_web/static/teasers/raytrace/seg134763_19_vol_rgb.jpg" alt="Volume-rendered image"/>
          <div class="content has-text-centered">
            <p>Volume-rendered image</p>
          </div>
        </div>
        <div class="column is-one-quater">
          <img src="streetsurf_web/static/teasers/raytrace/seg134763_19_sphere_rgb.jpg" alt="Sphere-traced image"/>
          <div class="content has-text-centered">
            <p>Sphere-traced image</p>
          </div>
        </div>
      </div>


      <!-- <div class="content has-text-justified">
        <p>
          Shade the implicit surface under different envmaps based on sphere-tracing.
        </p>
      </div> -->

    </div>
  </div>
</section>
<!-- End Ray tracing -->

<!-- Benchmark -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Benchmark of street-view static reconstruction in Waymo Open Dataset</h2>

    </div>
  </div>
</section> -->
<!-- End Benchmark -->

<!-- Paper poster -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    @article{guo2023streetsurf,
      title={StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views},
      author={Guo, Jianfei and Deng, Nianchen and Li, Xinyang and Bai, Yeqi and Shi, Botian and Wang, Chiyu and Ding, Chenjing and Wang, Dongliang and Li, Yikang},
      journal={arXiv preprint arXiv:2306.04988},
      year={2023}
    }
  </code></pre>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built based on the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> and the source code of the <a href="https://nerfies.github.io/" target="_blank">nerfies</a> website.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
